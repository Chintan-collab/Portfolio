<!DOCTYPE HTML><html><head><title>Chiintan's Portfolio</title><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no"><link rel="stylesheet" href="index.3b39d34f.css"><noscript> <link rel="stylesheet" href="index.e7532048.css"> </noscript></head><body class="is-preload"> <div id="page-wrapper">  <header id="header"> <h3 id="logo"><a href="index.html"></a></h3> <nav id="nav"> <ul> <li><a href="index.html">Home</a></li> <li> <a href="#">Projects</a> <ul> <li><a href="IDEAC.html">IDEAC</a></li> <li><a href="CloudSync.html">CloudSync</a></li> <li><a href="Netra.html">Netra-i</a></li> <li><a href="Falcon.html">Falcon</a></li> <li><a href="Synthos.html">Synthos</a></li> </ul> </li> <li> <a href="#">Navigate</a> <ul> <li><a href="profile.html">Profile</a></li> <li><a href="assets/pdfs/Chintan%27s%20CV.pdf" download="Chintan's Resume.pdf">Resume</a></li> <li> <a href="#">Academics</a> <ul> <li><a href="Profile_Summary.html#skills">Skills Summary</a></li> <li><a href="Profile_Summary.html#work">Work Experience</a></li> <li><a href="Profile_Summary.html#testimonials">Testimonials</a></li> <li><a href="Profile_Summary.html#education">Education</a></li> <li><a href="Profile_Summary.html#achievements">Awards and Achievements</a></li> </ul> </li> </ul> </li> </ul> </nav> </header>  <div id="main" class="style1 wrapper"> <div class="container"> <header class="major"> <h2>Falcon - Reveal the Depths of ML: Uncover Insights with Falcon</h2> <p style="text-align:center">Seamlessly Bridging Model Monitoring and Interpretation for Informed Access.</p> </header>  <section id="content"> <a href="#" class="fit image"><img src="falcon.f4e94c98.jpg" alt style="width:81vw;height:80vh"></a> <h3>Brief Overview</h3> <p>The purpose of model monitoring lies in the critical task of controlling and evaluating the performance of machine learning (ML) models to ensure their efficient operation. In essence, it serves as a proactive measure to detect and address any performance decay that may occur over time. Analogous to regular maintenance for a car, where one replaces tires or changes the oil to enhance performance, model monitoring aims to keep ML models running smoothly. </p> <h3>Objectives:</h3> <ul> <li>In today's data-driven business landscape, many companies rely on ML applications to make strategic decisions. However, the inherent challenge is that the performance of ML models naturally degrades as they continue to operate. This degradation can have far-reaching consequences, including suboptimal decisions, declining profits, and reduced revenue, which can significantly impact a company's bottom line.</li> <li>To mitigate this potential detriment, companies must establish a performance threshold for their ML models, treating it as a key performance indicator (KPI) that must always be met. This necessitates the regular and vigilant monitoring of ML models to ensure they remain effective and aligned with organizational goals.</li> <li>Yet, the reality is that, all too often, organizations are unaware of their models' performance issues until users raise complaints. Unfortunately, by the time these complaints surface, it is often too late to avert the resulting damage. Organizations find themselves in a scramble, transitioning into full damage control mode, as they strive to identify the root causes of issues and rectify broken models, all while attempting to mollify frustrated users.</li> <li>In summary, the purpose and goals of model monitoring are twofold: to proactively maintain the efficiency of ML models and to prevent the potential negative consequences that arise from unchecked performance decay. It is an essential practice for companies seeking to leverage ML in their decision-making processes while ensuring that users consistently experience optimal performance.</li> </ul> <h3>Background:</h3> <p>In the context of my IDEAC system, monitoring becomes crucial to maintaining the system's effectiveness over time. Several key points pertain to why and how this monitoring is essential: </p> <ul> <ul><b>Evolving Document Content:</b> <li>Documents in the real world are dynamic; their content changes over time.</li> <li>Without monitoring, IDEAC could become outdated and misclassify documents as the language and topics evolve.</li> </ul> <ul><b>Changing Terminology:</b> <li>Language evolves, leading to shifts in terminology and phrasing.</li> <li>Monitoring ensures the system can classify these new document types accurately.</li> </ul> <ul><b>New Document Types::</b> <li>As businesses and industries evolve, new types of documents may emerge.</li> <li>Without monitoring, IDEAC could become outdated and misclassify documents as the language and topics evolve.</li> </ul> <ul><b>Data Drift:</b> <li>Data sources and input data can change over time.</li> <li>Monitoring helps detect and address data drift, ensuring the system remains effective. </li> </ul> <ul><b>Concept Drift:</b> <li>The concepts or topics discussed in documents may shift.</li> <li>Regular monitoring allows the system to adapt to changing document themes and topics. </li> </ul> <ul><b>Performance Decay:</b> <li>Without monitoring, the performance of IDEAC can degrade over time.</li> <li>This can lead to misclassification, reduced efficiency, and potentially costly errors. </li> </ul> <ul><b>Ensuring Relevance and Avoiding Staleness:</b> <li>Continuous monitoring ensures that IDEAC remains relevant to the current business needs and objectives.</li> <li>IDEAC relying solely on historical training data may become "stale" and less accurate. Ongoing monitoring prevents this staleness by enabling updates based on current data. </li> </ul> </ul> <h3>Approach:</h3> <ul> <ul><b>Create a WebApp with Two Panels: "Annotator" and "Observer"</b> <li>Develop a Flask or Django based web application with two distinct user interfaces: one for the annotator and another for the Observer.</li> <li>Implement secure user authentication and role-based access control to ensure data privacy and access restrictions.</li> </ul> <ul><b>Annotator Panel User Interface</b> <li>Design a user-friendly interface for the annotator panel.</li> <li>Display a template for annotating PDF files, presenting predicted labels, document text, and options for "Yes" and "No" for the predicted label's correctness.</li> <li>If the user selects "No," provide a dropdown menu with all possible labels for manual correction.</li> <li>Enable form submission for annotating PDFs and seamlessly present the next document for annotation.</li> </ul> <ul><b>Observer Panel for Report Generation</b> <li>Develop the observer panel with functionalities to generate reports for specific time periods based on the annotated data provided by the annotator.</li> <li>Implement report generation on a weekly basis to track the system's performance and changes over time.</li> </ul> <ul><b>Central Database</b> <li>Set up a centralized database to store and manage prediction data, annotator feedback, training data, and model versions.</li> <li>Ensure data integrity, security, and scalability in the database design.</li> <li>Include features for data versioning and maintaining a historical record of changes. </li> </ul> <ul><b>Model Retraining and Versioning</b> <li>Establish a workflow for model retraining based on annotated data.</li> <li>Implement mechanisms to trigger model retraining when a sufficient volume of annotated data is available.</li> <li>Automatically push retrained models to an S3 bucket with new version labels.</li> </ul> <ul><b>Engineer Validation</b> <li>Enable the engineer to validate and assess the performance of retrained models.</li> <li>Provide access to training reports and model evaluation metrics for the engineer's review.</li> <li>Implement model validation and performance tracking as part of the monitoring system. </li> </ul> <ul><b>Report Export Option</b> <li>Offer the observer the ability to export reports in a convenient format (e.g., PDF or HTML).</li> <li>Provide an option to download all reports for a specified time period in a zip file for offline viewing and archival purposes.</li> </ul> <ul><b>Text Instance Interpretation with SMOTE and BERT Models</b> <li>Incorporate SMOTE and BERT (Bidirectional Encoder Representations from Transformers) models for text instance interpretation.</li> <li>Enhance the monitoring system's ability to handle imbalanced data and improve classification accuracy.</li> </ul> <ul><b>Testing and Quality Assurance</b> <li>Conduct thorough testing, including unit testing, integration testing, and user acceptance testing, to ensure the system's robustness and functionality.</li> <li>Implement error handling and logging mechanisms to identify and address issues promptly. </li> </ul> <ul><b>Security and Compliance</b> <li>Implement robust security measures to protect sensitive data and ensure compliance with data privacy regulations. Regularly audit and update security protocols to address emerging threats and vulnerabilities.</li> </ul> </ul> <h3>Feature Review</h3>  <div class="video-container" style="width:81vw;height:80vh"> <video controls width="100%" height="100%"> <source src="CloudFalcon.b28b0360.mp4" type="video/mp4"> </video> </div> <br> <h3>Next Steps:</h3> <ul> <li>As the monitoring system evolves, we plan to focus on further improving the retraining process. One of the next steps is to implement a feature that allows model retraining to be triggered seamlessly from the monitoring interface. This will involve integrating the monitoring system with our model retraining pipeline, enabling quick and efficient updates to the classification models in response to changing document content and linguistic shifts.</li> <li>This recommendation aligns with our overarching goal of proactively maintaining the efficiency of our ML models and preventing performance decay. It will ensure that the system remains aligned with our organizational goals and continues to provide accurate document classification.</li> </ul> <h3>Further Research and Development:</h3> <ul> <li><b>Expanding Model Retraining Capabilities: </b> While our current monitoring system will primarily focus on the retraining of the main classifier, the next phase of our research and development will involve providing the capability to retrain any of the models within our system. This expanded retraining functionality will offer more flexibility in adapting to the evolving needs of our document classification process. By enabling users to select and retrain specific models, I aim to address the challenges associated with concept drift and data drift across the spectrum of our machine learning models.</li> <li><b>Auto-Versioning of Artifacts in S3:</b> A crucial aspect of maintaining data integrity and system scalability is the automatic versioning of artifacts stored in our Amazon S3 bucket. In our ongoing development, I plan to implement an automated version control system that seamlessly tracks and manages changes in model versions, annotated data, and other relevant resources. This feature will enhance the traceability and reproducibility of our machine learning pipeline, ensuring that historical records are preserved while simplifying the process of managing resources over time.</li> </ul> </section> </div> </div>  <footer id="footer"> <ul class="icons"> <li><a href="https://www.linkedin.com/in/chintan-vekariya-633278190/" class="alt brands fa-linkedin-in icon"><span class="label">LinkedIn</span></a></li> <li><a href="https://github.com/Chintan-collab" class="alt brands fa-github icon"><span class="label">GitHub</span></a></li> <li><a href="mailto:chiintan.vekariya@gmail.com" class="alt fa-envelope icon solid"><span class="label">Email</span></a></li> </ul> </footer> </div>  <script src="index.471e94a0.js"></script> <script src="index.85c18fe2.js"></script> <script src="index.0d0f8b0f.js"></script> <script src="index.b9c269a9.js"></script> <script src="index.af0c9ec1.js"></script> <script src="index.820333a9.js"></script> <script src="index.06066817.js"></script> <script src="index.61ceb025.js"></script> </body></html>